import pandas as p
import numpy as np
import matplotlib.pyplot as plt

# Verify the pandas has been imported
df = p.Dataframe({"A":[11,12,13], "B":[34,78.109]})
df.ssum()

# Load the train and test datasets
train= p.read_csv("E:\\labs\\rakshitlabs\\PredictingKaggleHouseDataset\\trainingHousingDataset\\data\\all\\train.csv")
test= p.read_csv("E:\\labs\\rakshitlabs\\PredictingKaggleHouseDataset\\trainingHousingDataset\\data\\all\\test.csv")

print("Train data shape:", train.shape)
print("Test data shape:", test.shape)

# Peek at the data
train.head()

# Import matplotlib for plotting graphs(./python -mpip -U matplotlib)
print("Test data shape:", test.shape)

# Describe the target variable. In this scenario the target variable is the SalePrice, which we are going to predict.
train.SalePrice.describe()

# Next check the skewness which will describe the shape of the distribution of the values.
# To improve the linearity sometimes the target variable needs to be log-transformed(np.log()).
# If so, then the predictions generated by the final model would also be log-transformed.
# This will be later needed to transform back to its original form(np.exp()).
print("Skew is: ", train.SalePrice.skew())
plt.hist(train.SalePrice, color='blue')
plt.show()

# Log-transform the target variable to align towards normal distribution
target= np.log(train.SalePrice)
print('Skew after log-transformation is:', target.skew())
plt.hist(target, color='blue')
plt.show()

## Feature Selection and Analysis
# Working with Numeric Features
numeric_features= train.select_dtypes(include=[np.number])
numeric_features.dtypes

# Get the correlation of the features
corr= numeric_features.corr()

# Top 5 positively correlated
print(corr['SalePrice'].sort_values(ascending=False)[:5], '\n')

# Top 5 negatively correlated
print(corr['SalePrice'].sort_values(ascending=False)[-5:])

# The col 'OverallQual' is the most correlated with the target variable.
# Unique values of 'OverallQual'
train.OverallQual.unique()

# The unique values are 1-10 inclusive. Using pivot-table to analyse the median
quality_pivot= train.pivot_table(index= 'OverallQual', values= 'SalePrice', aggfunc= np.median)

# Visualise the pivot-table
quality_pivot.plot(kind='bar', color='blue')
plt.xlabel('Overall Quality')
plt.ylabel('Median Sale Price')
plt.xticks(rotation=0)
plt.show()

# Plotting scatterplot against the Ground Living Area and Sale Price
plt.scatter(x=train['GrLivArea'], y=target)
plt.ylabel('Sale Price')
plt.xlabel('Above grade (ground) living area square feet')
plt.show()

# Plotting Scatterplot against GarageArea and SalePrice
plt.scatter(x=train['GarageArea'], y=target)
plt.ylabel('Sale Price')
plt.xlabel('GarageArea')
plt.show()

# Removing outliers and trying again.
# Outliers can affect a regression model by pulling our estimated regression line further away from the true population regression line.
train= train[train['GarageArea'] < 1200]
plt.scatter(x=train['GarageArea'], y=np.log(train.SalePrice))
plt.xlim(-200,1600)
plt.xlabel('Garage Area')
plt.ylabel('Sale Price')
plt.show()

## Handling null values
# Create a DataFrame to handle all the null values.
nulls= p.DataFrame(train.isnull().sum().sort_values(ascending=False)[:25])
nulls.columns= ['Null Count']
nulls.index.name = 'Feature'
nulls

# See the unique values of column 'MiscFeature'.
print('Unique values of MiscFeature are:', train.MiscFeature.unique())

# Wrangling the non-numeric feature
categoricals= train.select_dtypes(exclude=[np.number])
categoricals.describe()

# One-Hot encoding might be required to make some transformation for modelling.
# Any Transformation done on the training dataset, must be applied to the test data set as well.
print('Original: \n')
print(train.Street.value_counts(), "\n")


## Feature Engineering with one-hot encoding
# Engineering the Street feature into a column of boolean feature 'enc_street' with get_dummies() method.
train['enc_street']= p.get_dummies(train.Street, drop_first= True)
test['enc_street'] = p.get_dummies(train.Street, drop_first=True)
print('Encode: \n')
print(train.enc_street.value_counts())

# Engineering the Sale Condition feature by plotting a pivot table
# Without encoding
condition_pivot = train.pivot_table(index='SaleCondition',values='SalePrice', aggfunc=np.median)
condition_pivot.plot(kind='bar', color='blue')
plt.xlabel('Sale Condition')
plt.ylabel('Median Sale Price')
plt.xticks(rotation=0)
plt.show()

# With Encoding
# From the above plot we see Partial has significantly larger value than the others.
# Hence, a new derived col will be created labelling values with Partial as 1 rest as 0 with the encode() func.
def encode(x): return 1 if x == 'Partial' else 0
train['enc_condition']= train.SaleCondition.apply(encode)
test['enc_condition']= test.SaleCondition.apply(encode)
condition_pivot = train.pivot_table(index='enc_condition', values='SalePrice', aggfunc=np.median)
condition_pivot.plot(kind='bar', color='blue')
plt.xlabel('Encoded Sale Condition')
plt.ylabel('Median Sale Price')
plt.xticks(rotation=0)
plt.show()

# handling null values or missing data with DataFrame.interpolation() method.
data = train.select_dtypes(include=[np.number]).interpolate().dropna()

# Verify if the cols doesn't have any null/nan values
sum(data.isnull().sum() != 0)

## Building the linear model
# Differentiate the features with the target by labelling them with X and Y correspondingly.
# Log-transform the target variable as mentioned above.
y= np.log(train.SalePrice)
x= data.drop(['SalePrice', 'Id'], axis=1) # dropping the non-relevant features

# analyse the over-fitting with split test data train_test_split() from scikit-learn()
# 33% of the data is used as test data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test= train_test_split(x,y, random_state=42, test_size=.33)

# LinearRegression model building
# Instantiate the model
from sklearn import linear_model
lr= linear_model.LinearRegression()


# Fit the model. Estimate the relationship of the features and the target variable
















